{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfc0e42-7919-47f9-817e-9b630fe98ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b9114-569a-4730-bc48-276072d934df",
   "metadata": {},
   "source": [
    "# Appendix: Inspecting data\n",
    "\n",
    "### 0) Importing all the data\n",
    "* We import randomly the data into a training set, validation set and test set, ensuring that the same mouse models do not occur in the different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef57643c-7236-4754-9e0d-3bbaa556ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data folder\n",
    "data_folder = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be15d21d-7006-4973-a335-5a31aee5b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking csv-files\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# making a dictionary based on the genome letter in the mouse\n",
    "group_dict = {}\n",
    "for filename in csv_files:\n",
    "    # splits out extension and get the genome name\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    group_name = base_name[-5:]\n",
    "    # storing the filename with the right genome\n",
    "    group_dict.setdefault(group_name, []).append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5347d324-b291-402d-bc60-a103c0c9f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the data\n",
    "group_names = list(group_dict.keys())\n",
    "random.shuffle(group_names)\n",
    "\n",
    "# creating train, val and test sets\n",
    "num_groups = len(group_names)\n",
    "train_size = int(0.6 * num_groups)\n",
    "test_size = int(0.2 * num_groups)\n",
    "val_size = num_groups - train_size - test_size\n",
    "\n",
    "train_groups = group_names[:train_size]\n",
    "test_groups = group_names[train_size:train_size + test_size]\n",
    "val_groups = group_names[train_size + test_size:]\n",
    "\n",
    "train_files = [filename for group in train_groups for filename in group_dict[group]]\n",
    "test_files = [filename for group in test_groups for filename in group_dict[group]]\n",
    "val_files = [filename for group in val_groups for filename in group_dict[group]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74d9c0b-0675-4a2f-9e17-2dbbbe97be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that filters out the 'unknown' data\n",
    "\n",
    "def load_and_filter(files_list):\n",
    "    dfs = []\n",
    "    for filename in files_list:\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        # filters out unknown\n",
    "        df = df[df['sleep_episode'] != 0] # unknown = 0\n",
    "        # removes the time column\n",
    "        df = df.drop(columns=['time'])\n",
    "        dfs.append(df)\n",
    "    # creates one long file with all the sleep scoring\n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # for empty dfs\n",
    "\n",
    "train_df = load_and_filter(train_files)\n",
    "test_df = load_and_filter(test_files)\n",
    "val_df = load_and_filter(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ea565-6beb-45ff-b937-bd95f92e2ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf1c86c-12de-4ffb-b838-0c44fe80d240",
   "metadata": {},
   "source": [
    "### 1) Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb822f4-17e2-4117-95d1-73e60656f2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of          delta_power  theta_power  sigma_power  beta_power  emg_power  \\\n",
       "0           0.035697     0.050050     0.024376    0.020170   0.012657   \n",
       "1           0.038797     0.047070     0.020935    0.019491   0.031251   \n",
       "2           0.030161     0.062763     0.023212    0.015587   0.018868   \n",
       "3           0.041064     0.056119     0.023254    0.022701   0.036574   \n",
       "4           0.055847     0.047321     0.026141    0.019780   0.029522   \n",
       "...              ...          ...          ...         ...        ...   \n",
       "1167096     0.054960     0.034170     0.013552    0.013920   0.097897   \n",
       "1167097     0.069506     0.032111     0.021751    0.013156   0.092292   \n",
       "1167098     0.047995     0.038328     0.016894    0.015349   0.092578   \n",
       "1167099     0.054761     0.044430     0.021102    0.017827   0.092924   \n",
       "1167100     0.074287     0.038952     0.017136    0.013118   0.094564   \n",
       "\n",
       "         sleep_episode  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  \n",
       "...                ...  \n",
       "1167096              1  \n",
       "1167097              1  \n",
       "1167098              1  \n",
       "1167099              1  \n",
       "1167100              1  \n",
       "\n",
       "[1167101 rows x 6 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab7f8b-dd61-4969-a23f-0d01fdbbfad2",
   "metadata": {},
   "source": [
    "### 2) Statistical summary\n",
    "\n",
    "#### Theoretical background\n",
    "\n",
    "- **Mean** ($\\mu$): The average value of a dataset, calculated as:\n",
    "  $$\n",
    "  \\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i\n",
    "  $$\n",
    "\n",
    "- **Standard Deviation** ($\\sigma$): A measure of data dispersion around the mean:\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2}\n",
    "  $$\n",
    "\n",
    "- **Variance** ($\\sigma^2$): The square of the standard deviation:\n",
    "  $$\n",
    "  \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9339af8b-98a2-4910-8417-043edb9734e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta_power</th>\n",
       "      <th>theta_power</th>\n",
       "      <th>sigma_power</th>\n",
       "      <th>beta_power</th>\n",
       "      <th>emg_power</th>\n",
       "      <th>sleep_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.167101e+06</td>\n",
       "      <td>1.167101e+06</td>\n",
       "      <td>1.167101e+06</td>\n",
       "      <td>1.167101e+06</td>\n",
       "      <td>1.167101e+06</td>\n",
       "      <td>1.167101e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.911652e-02</td>\n",
       "      <td>4.547190e-02</td>\n",
       "      <td>2.515342e-02</td>\n",
       "      <td>2.127904e-02</td>\n",
       "      <td>1.647124e-01</td>\n",
       "      <td>1.538742e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.558375e-02</td>\n",
       "      <td>2.922236e-02</td>\n",
       "      <td>1.645387e-02</td>\n",
       "      <td>1.113252e-02</td>\n",
       "      <td>2.390643e-01</td>\n",
       "      <td>7.772455e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.463106e-01</td>\n",
       "      <td>-1.914635e-01</td>\n",
       "      <td>-8.540071e-02</td>\n",
       "      <td>-4.784966e-02</td>\n",
       "      <td>-4.138887e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.653000e-02</td>\n",
       "      <td>3.064264e-02</td>\n",
       "      <td>1.568016e-02</td>\n",
       "      <td>1.448491e-02</td>\n",
       "      <td>5.256403e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.823673e-02</td>\n",
       "      <td>3.936916e-02</td>\n",
       "      <td>2.078475e-02</td>\n",
       "      <td>1.858644e-02</td>\n",
       "      <td>9.279487e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.688062e-02</td>\n",
       "      <td>5.316248e-02</td>\n",
       "      <td>2.915336e-02</td>\n",
       "      <td>2.505387e-02</td>\n",
       "      <td>1.604122e-01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.277576e+00</td>\n",
       "      <td>1.592684e+00</td>\n",
       "      <td>7.065098e-01</td>\n",
       "      <td>4.789401e-01</td>\n",
       "      <td>5.767568e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        delta_power   theta_power   sigma_power    beta_power     emg_power  \\\n",
       "count  1.167101e+06  1.167101e+06  1.167101e+06  1.167101e+06  1.167101e+06   \n",
       "mean   4.911652e-02  4.547190e-02  2.515342e-02  2.127904e-02  1.647124e-01   \n",
       "std    7.558375e-02  2.922236e-02  1.645387e-02  1.113252e-02  2.390643e-01   \n",
       "min   -3.463106e-01 -1.914635e-01 -8.540071e-02 -4.784966e-02 -4.138887e-01   \n",
       "25%    2.653000e-02  3.064264e-02  1.568016e-02  1.448491e-02  5.256403e-02   \n",
       "50%    3.823673e-02  3.936916e-02  2.078475e-02  1.858644e-02  9.279487e-02   \n",
       "75%    5.688062e-02  5.316248e-02  2.915336e-02  2.505387e-02  1.604122e-01   \n",
       "max    3.277576e+00  1.592684e+00  7.065098e-01  4.789401e-01  5.767568e+00   \n",
       "\n",
       "       sleep_episode  \n",
       "count   1.167101e+06  \n",
       "mean    1.538742e+00  \n",
       "std     7.772455e-01  \n",
       "min     1.000000e+00  \n",
       "25%     1.000000e+00  \n",
       "50%     1.000000e+00  \n",
       "75%     2.000000e+00  \n",
       "max     4.000000e+00  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bbb1a-f16e-4934-b954-0fd3afa498bc",
   "metadata": {},
   "source": [
    "#### Observations in the data set\n",
    "**Count:** Each variable has 1,148,392 data points, indicating no missing values across the columns.\\\n",
    "**Means:** All the power variables have low mean values, suggesting the data is centered around small magnitudes. \n",
    "\n",
    "Note: The low value of `sleep_episode` mean (1.57) indicates that a majority of episodes belong to `Wake (class 1)`.\n",
    "\n",
    "**Standard deviations:** `emg_power` is the power variable with the highest std. This suggests that the muscle waves have a broader set of values than the brain waves: they latter are more tightly centered around their mean.\n",
    "\n",
    "### 3) Possible missing values\n",
    "\n",
    "As we saw in 2) there seem to be no missing or NaN-values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2414567-8c07-4607-8271-e8ef5b2c2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = train_df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc43716-aafa-45f3-8d6b-aa8a43fe5b21",
   "metadata": {},
   "source": [
    "### 4) Examining class balance and weighting\n",
    "\n",
    "In imbalanced datasets, certain classes dominate the training process, leading to biased models. To mitigate this issue, we calculate weights for each class based on their frequencies. The weights are defined as inversely proportional to the frequency of each class, ensuring that underrepresented classes are given higher importance:\n",
    "\n",
    "$$\n",
    "w_c = \\frac{1}{n_c}\n",
    "$$\n",
    "\n",
    "where $n_c$ is the number of samples in class $c$.\n",
    "\n",
    "These weights are then normalized to sum to 1:\n",
    "\n",
    "$$\n",
    "\\tilde{w}_c = \\frac{w_c}{\\sum_{c=1}^{C} w_c}\n",
    "$$\n",
    "This ensures that the underrepresented classes are given higher importance during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b59ca-43b9-4221-8bd2-518a4a07da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = train_df['sleep_episode'].value_counts()\n",
    "class_weights = 1/class_counts\n",
    "normalized_weights = class_weights / class_weights.sum()\n",
    "print(\"Class balance\\n\", class_counts)\n",
    "print(\"\\nClass weights (normalized)\\n\", normalized_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d87aa-a751-477f-ad92-f98ad514c0a8",
   "metadata": {},
   "source": [
    "### 5) Correlation Analysis for Further Intuition\n",
    "\n",
    "A **correlation matrix** provides an overview of the linear relationships between features in a dataset. The values in the matrix range from $-1$ to $1$, where:\n",
    "\n",
    "- Values close to $1$ indicate a strong positive relationship: as one feature increases, so does the other.\n",
    "- Values close to $-1$ indicate a strong negative relationship: as one feature increases, the other decreases.\n",
    "- Values close to $0$ indicate little to no linear relationship between the features.\n",
    "\n",
    "Features that are highly correlated could provide redundant information to the model. By identifying these, we could simplify the dataset without losing valuable information, potentially improving model efficiency and interpretability.\n",
    "\n",
    "[Reference](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter8.html#correlation-matrix)\n",
    "\n",
    "**Pair Plots for Visual Inspection**\n",
    "\n",
    "To complement the correlation matrix, pair plots can be used to visualize pairwise relationships between features. The pair plot creates scatter plot between every two variables in our dataset (source: https://seaborn.pydata.org/generated/seaborn.pairplot.html). If the data is highly correlated the plot will be more diagonal, and in the opposite case the data forms other \"lump-like\" structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e3cd3-a932-44fb-a15c-054cda50e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a directory to save the plots in\n",
    "output_folder = 'plots'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68dbfc-a171-4da9-a098-da23846e1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LaTeX font style and font size\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.size\": 16,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_df.drop(columns=['sleep_episode']).corr()\n",
    "\n",
    "# Update class labels\n",
    "class_labels = [r'$\\delta$', r'$\\theta$', r'$\\sigma$', r'$\\beta$', 'EMG']\n",
    "class_labels = ['Delta', 'Theta', 'Sigma', 'Beta', 'EMG']\n",
    "\n",
    "# Create heatmap with updated labels\n",
    "plt.figure(figsize=(6, 8))  # Adjust to keep the plot square\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    cbar=None,\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    square=True  # Ensures the cells are square\n",
    ")\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig(os.path.join(output_folder, 'corr_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be123cef-9c53-45e1-b084-e2cd33df86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a test fraction of 10% of the entire training set\n",
    "sample_fraction = 0.1\n",
    "sampled_df = train_df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# plotting pairplot\n",
    "pair_plot = sns.pairplot(sampled_df, hue='sleep_episode', markers=[\"o\", \"s\", \"D\", \"*\"])\n",
    "pair_plot.savefig(os.path.join(output_folder, 'pairplot.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce73cb-561d-4b20-ad63-ced0b14ad86d",
   "metadata": {},
   "source": [
    "From this plot we can read that `theta_power` shares data with `sigma_power` and `beta_power`, and the same is true within `sigma_power` and `beta_power`. It *could* then be that beta and sigma are not good predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d5a2f-2258-4433-b4a1-c888eed273ed",
   "metadata": {},
   "source": [
    "### 6) Further examining feature importance with a Random Forest Classifier\n",
    "\n",
    "We could gain further insight into the importance of each predictor by quickly training an off the shelf Random Forest network. \n",
    "\n",
    "Random Forests are ensemble learning methods that build multiple **decision trees** during training and aggregate their results to improve predictions. They are particularly useful for examining feature importance.\n",
    "\n",
    "The impurity reduction, often measured using the Gini impurity or entropy, is calculated as:\n",
    "$$\n",
    "\\text{Impurity Reduction} = \\text{Impurity}_{\\text{parent}} - \\sum_{i} \\frac{N_i}{N_{\\text{parent}}} \\cdot \\text{Impurity}_{\\text{child}, i}\n",
    "$$\n",
    "where $N_i$ is the number of samples in the child node and $N_{\\text{parent}}$ is the number of samples in the parent node.\n",
    "\n",
    "Feature importance is aggregated over all splits and all trees in the forest. Features that appear frequently at the top levels of trees or significantly reduce impurity are assigned higher importance.\n",
    "\n",
    "[Reference](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/week46.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd256b4-8d7b-4249-8a13-0d8cbb322916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(sampled_df[['delta_power', 'theta_power', 'sigma_power', 'beta_power', 'emg_power']], sampled_df['sleep_episode'])\n",
    "feature_importances = rf.feature_importances_\n",
    "print(dict(zip(['delta_power', 'theta_power', 'sigma_power', 'beta_power', 'emg_power'], feature_importances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f224598-c0df-47f7-bc2d-49387d389a7c",
   "metadata": {},
   "source": [
    "From this result, we can see that `delta_power` is the most significant predictor, followed by `emg_power`and `sigma_power`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42312372-a050-4f52-a10c-f284f2355a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Velg numeriske kolonner som skal normaliseres\n",
    "numerical_cols = ['delta_power', 'theta_power', 'sigma_power', 'beta_power', 'emg_power']\n",
    "\n",
    "# Opprett og tren en MinMaxScaler på treningsdataene\n",
    "scaler = MinMaxScaler()\n",
    "train_df[numerical_cols] = scaler.fit_transform(train_df[numerical_cols])\n",
    "\n",
    "# Bruk den trenede skaleren på validerings- og testdataene\n",
    "val_df[numerical_cols] = scaler.transform(val_df[numerical_cols])\n",
    "test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1aa6c-6291-4134-8c0f-8d8df8a3969b",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51427f2e-a456-479c-975c-4d66a8a833b6",
   "metadata": {},
   "source": [
    "### 7) Creating power band ratios\n",
    "\n",
    "With the intuition gained in 5) and 6), we could now remove some of the \"raw\" powerbands, and add some ratios, namely `delta_power/theta_power`, `sigma_power/beta_power`, `theta_power/sigma_power` and `emg_power/delta_power` to the data set. Then we could do another correlation matrix, to see if this has improved the model's predictors.\n",
    "\n",
    "Creating ratios between features can normalize the data and highlight relationships. For example:\n",
    "$$\n",
    "\\text{delta\\_theta\\_ratio} = \\frac{\\text{delta\\_power}}{\\text{theta\\_power}}\n",
    "$$\n",
    "Ratios can reduce scale dependencies and emphasize relative changes, which are often more informative for classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9622fa-f1c9-49e2-85be-f82a7c57240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['delta_theta_ratio'] = train_df['delta_power'] / train_df['theta_power']\n",
    "val_df['delta_theta_ratio'] = val_df['delta_power'] / val_df['theta_power']\n",
    "test_df['delta_theta_ratio'] = test_df['delta_power'] / test_df['theta_power']\n",
    "\n",
    "train_df['sigma_beta_ratio'] = train_df['sigma_power'] / train_df['beta_power']\n",
    "val_df['sigma_beta_ratio'] = val_df['sigma_power'] / val_df['beta_power']\n",
    "test_df['sigma_beta_ratio'] = test_df['sigma_power'] / test_df['beta_power']\n",
    "\n",
    "train_df['theta_sigma_ratio'] = train_df['theta_power'] / train_df['sigma_power']\n",
    "val_df['theta_sigma_ratio'] = val_df['theta_power'] / val_df['sigma_power']\n",
    "test_df['theta_sigma_ratio'] = test_df['theta_power'] / test_df['sigma_power']\n",
    "\n",
    "train_df['emg_delta_ratio'] = train_df['emg_power'] / train_df['delta_power']\n",
    "val_df['emg_delta_ratio'] = val_df['emg_power'] / val_df['delta_power']\n",
    "test_df['emg_delta_ratio'] = test_df['emg_power'] / test_df['delta_power']\n",
    "\n",
    "# moving 'sleep_episode' to the back\n",
    "train_df = train_df[[col for col in train_df.columns if col != 'sleep_episode'] + ['sleep_episode']]\n",
    "val_df = val_df[[col for col in val_df.columns if col != 'sleep_episode'] + ['sleep_episode']]\n",
    "test_df = test_df[[col for col in test_df.columns if col != 'sleep_episode'] + ['sleep_episode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288bd29-4928-44a1-a2ff-286b08bc2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08388c0f-c56c-4cd5-8353-3cbde1e52f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing \"raw\" variables\n",
    "columns_to_remove = ['theta_power', 'beta_power']\n",
    "#columns_to_remove = ['delta_power', 'theta_power', 'beta_power']\n",
    "\n",
    "train_df = train_df.drop(columns=columns_to_remove)\n",
    "test_df = test_df.drop(columns=columns_to_remove)\n",
    "val_df = val_df.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9d74f-197a-4c95-8f4b-dbcccd519c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655c3e6-56f2-4bb8-bf7c-c6611f37df37",
   "metadata": {},
   "source": [
    "### 8) New plots after feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd398a-b18d-4422-b7fe-67ffda4df04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fraction = 0.1\n",
    "sampled_df = train_df.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# pair plot\n",
    "pair_plot = sns.pairplot(sampled_df, hue='sleep_episode', markers=[\"o\", \"s\", \"D\", \"*\"])\n",
    "pair_plot.savefig(os.path.join(output_folder, 'pairplot_after_engineering.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f00ab-2416-42bf-8acc-821e41a85c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set LaTeX font style and font size\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.size\": 16,\n",
    "    \"font.family\": \"serif\",\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_df.drop(columns=['sleep_episode']).corr()\n",
    "\n",
    "# Update class labels\n",
    "class_labels = [r'$\\delta$', r'$\\theta$', r'$\\sigma$', r'$\\beta$', 'EMG']\n",
    "class_labels = ['Delta', 'Sigma', 'EMG', r'$\\delta/\\theta$', r'$\\sigma/\\beta$', r'$\\theta/\\sigma$', r'EMG$/\\delta$']\n",
    "\n",
    "# Create heatmap with updated labels\n",
    "plt.figure(figsize=(6, 8))  # Adjust to keep the plot square\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    cbar=None,\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    square=True  # Ensures the cells are square\n",
    ")\n",
    "\n",
    "# Save and show plot\n",
    "plt.savefig(os.path.join(output_folder, 'corr_matrix_after_engineering.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d74d9e-9b6d-4edd-9f55-4c7adf8d272c",
   "metadata": {},
   "source": [
    "### Future Investigations\n",
    "\n",
    "- Train models using only the most predictive raw features ($\\text{delta\\_power}, \\text{sigma\\_power}, \\text{emg\\_power}$).\n",
    "- Add power ratios (e.g., $\\frac{\\text{delta\\_power}}{\\text{theta\\_power}}$) and evaluate their impact.\n",
    "- Use **Principal Component Analysis (PCA)** to identify the most critical feature combinations, further reducing redundancy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (letten)",
   "language": "python",
   "name": "letten"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
